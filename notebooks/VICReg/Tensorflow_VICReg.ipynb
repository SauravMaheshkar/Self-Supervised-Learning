{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q3k-vKd0c8Uk"
   },
   "source": [
    "# 📦 Packages and Basic Setup\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QoNTJN31iTgb"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install -U rich tf-models-official\n",
    "\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "from rich import print\n",
    "import tensorflow_models as tfm\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from rich.progress import track\n",
    "from tensorflow.python.ops.numpy_ops import np_config\n",
    "\n",
    "from typing import Callable, Tuple, Any, List\n",
    "\n",
    "# Experimental options\n",
    "options = tf.data.Options()\n",
    "options.experimental_optimization.noop_elimination = True\n",
    "options.experimental_optimization.apply_default_optimizations = True\n",
    "options.experimental_deterministic = False\n",
    "options.threading.max_intra_op_parallelism = 1\n",
    "np_config.enable_numpy_behavior()\n",
    "\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "x0ak7lsXgKf5",
    "outputId": "d32269c8-1872-442e-e15c-426ab86c1cdb"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Random seed set as <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">42</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Random seed set as \u001b[1;36m42\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# @title ⚙ Configuration\n",
    "GLOBAL_SEED = 42  # @param {type: \"number\"}\n",
    "NUM_VIEWS = 2  # @param {type: \"number\"}\n",
    "NUM_EPOCHS = 10  # @param {type: \"number\"}\n",
    "BATCH_SIZE = 16  # @param {type: \"number\"}\n",
    "INVAR_COEFF = 25.0  # @param {type: \"number\"}\n",
    "VAR_COEFF = 25.0  # @param {type: \"number\"}\n",
    "COV_COEFF = 1.0  # @param {type: \"number\"}\n",
    "DECAY_STEPS = 1000  # @param {type: \"number\"}\n",
    "WEIGHT_DECAY = 1e-6  # @param {type: \"number\"}\n",
    "BASE_LR = 0.2  # @param {type: \"number\"}\n",
    "\n",
    "\n",
    "# ============ Random Seed ============\n",
    "def seed_everything(seed=GLOBAL_SEED):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    tf.experimental.numpy.random.seed(seed)\n",
    "    # When running on the CuDNN backend, two further options must be set\n",
    "    os.environ[\"TF_CUDNN_DETERMINISTIC\"] = \"1\"\n",
    "    os.environ[\"TF_DETERMINISTIC_OPS\"] = \"1\"\n",
    "    # Set a fixed value for the hash seed\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    print(f\"Random seed set as {seed}\")\n",
    "\n",
    "\n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PNagRfSfdAmO"
   },
   "source": [
    "# 🆘 Utility Classes and Functions\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B9o8vLp4w69C"
   },
   "outputs": [],
   "source": [
    "def off_diagonal(x: tf.Tensor) -> tf.Tensor:\n",
    "    n, m = x.shape[0], x.shape[1]\n",
    "    assert n == m, f\"Not a square tensor, dimensions found: {n} and {m}\"\n",
    "\n",
    "    flattened_tensor = tf.reshape(x, [-1])[:-1]\n",
    "    elements = tf.reshape(flattened_tensor, [n - 1, n + 1])[:, 1:]\n",
    "    return tf.reshape(elements, [-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wGDPpTX_dCl1"
   },
   "source": [
    "## 🖖 Utilites for Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zvPO2q_uj_f7"
   },
   "outputs": [],
   "source": [
    "GAUSSIAN_P = [1.0, 0.1]\n",
    "SOLARIZE_P = [0.0, 0.2]\n",
    "\n",
    "\n",
    "def shuffle_zipped_output(a: Any, b: Any) -> Tuple[Any]:\n",
    "    \"\"\"Shuffle the given inputs\"\"\"\n",
    "    listify = [a, b]\n",
    "    random.shuffle(listify)\n",
    "    return listify[0], listify[1]\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def scale_image(image: tf.Tensor, label: tf.Tensor) -> Tuple[tf.Tensor]:\n",
    "    \"\"\"Convert all images to float32\"\"\"\n",
    "    image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "    return (image, label)\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def gaussian_blur(\n",
    "    image: tf.Tensor, kernel_size: int = 23, padding: str = \"SAME\"\n",
    ") -> tf.Tensor:\n",
    "    \"\"\"\n",
    "    Randomly apply Gaussian Blur to the input image\n",
    "\n",
    "    Reference: https://github.com/google-research/simclr/blob/master/data_util.py\n",
    "    \"\"\"\n",
    "\n",
    "    sigma = tf.random.uniform((1,)) * 1.9 + 0.1\n",
    "    radius = tf.cast(kernel_size / 2, tf.int32)\n",
    "    kernel_size = radius * 2 + 1\n",
    "    x = tf.cast(tf.range(-radius, radius + 1), tf.float32)\n",
    "    blur_filter = tf.exp(\n",
    "        -tf.pow(x, 2.0) / (2.0 * tf.pow(tf.cast(sigma, tf.float32), 2.0))\n",
    "    )\n",
    "    blur_filter /= tf.reduce_sum(blur_filter)\n",
    "\n",
    "    # One vertical and one horizontal filter.\n",
    "    blur_v = tf.reshape(blur_filter, [kernel_size, 1, 1, 1])\n",
    "    blur_h = tf.reshape(blur_filter, [1, kernel_size, 1, 1])\n",
    "    num_channels = tf.shape(image)[-1]\n",
    "    blur_h = tf.tile(blur_h, [1, 1, num_channels, 1])\n",
    "    blur_v = tf.tile(blur_v, [1, 1, num_channels, 1])\n",
    "    expand_batch_dim = image.shape.ndims == 3\n",
    "    if expand_batch_dim:\n",
    "        image = tf.expand_dims(image, axis=0)\n",
    "    blurred = tf.nn.depthwise_conv2d(\n",
    "        image, blur_h, strides=[1, 1, 1, 1], padding=padding\n",
    "    )\n",
    "    blurred = tf.nn.depthwise_conv2d(\n",
    "        blurred, blur_v, strides=[1, 1, 1, 1], padding=padding\n",
    "    )\n",
    "    if expand_batch_dim:\n",
    "        blurred = tf.squeeze(blurred, axis=0)\n",
    "    return blurred\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def color_jitter(image: tf.Tensor, s: float = 0.5) -> tf.Tensor:\n",
    "    \"\"\"Randomly apply Color Jittering to the input image\"\"\"\n",
    "    x = tf.image.random_brightness(image, max_delta=0.8 * s)\n",
    "    x = tf.image.random_contrast(x, lower=1 - 0.8 * s, upper=1 + 0.8 * s)\n",
    "    x = tf.image.random_saturation(x, lower=1 - 0.8 * s, upper=1 + 0.8 * s)\n",
    "    x = tf.image.random_hue(x, max_delta=0.2 * s)\n",
    "    x = tf.clip_by_value(x, 0, 1)\n",
    "    return x\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def solarize(image: tf.Tensor, threshold: int = 128) -> tf.Tensor:\n",
    "    \"\"\"Solarize the input image\"\"\"\n",
    "    return tf.where(image < threshold, image, 255 - image)\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def color_drop(image: tf.Tensor) -> tf.Tensor:\n",
    "    \"\"\"Randomly convert the input image to GrayScale\"\"\"\n",
    "    image = tf.image.rgb_to_grayscale(image)\n",
    "    image = tf.tile(image, [1, 1, 3])\n",
    "    return image\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def random_apply(func: Callable, x: tf.Tensor, p: float) -> tf.Tensor:\n",
    "    \"\"\"Randomly apply the desired func to the input image\"\"\"\n",
    "    return tf.cond(\n",
    "        tf.less(\n",
    "            tf.random.uniform([], minval=0, maxval=1, dtype=tf.float32),\n",
    "            tf.cast(p, tf.float32),\n",
    "        ),\n",
    "        lambda: func(x),\n",
    "        lambda: x,\n",
    "    )\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def custom_augment(\n",
    "    image: tf.Tensor, label: tf.Tensor, gaussian_p: float = 0.1, solarize_p: float = 0.0\n",
    ") -> Tuple[tf.Tensor]:\n",
    "    \"\"\"Container function to apply all custom augmentations\"\"\"\n",
    "    # Random flips\n",
    "    image = random_apply(tf.image.flip_left_right, image, p=0.5)\n",
    "    # Randomly apply transformation (color distortions) with probability p.\n",
    "    image = random_apply(color_jitter, image, p=0.8)\n",
    "    # Randomly apply grayscale\n",
    "    image = random_apply(color_drop, image, p=0.2)\n",
    "    # Randomly apply gausian blur\n",
    "    image = random_apply(gaussian_blur, image, p=gaussian_p)\n",
    "    # Randomly apply solarization\n",
    "    image = random_apply(solarize, image, p=solarize_p)\n",
    "\n",
    "    return (image, label)\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def random_resize_crop(\n",
    "    image: tf.Tensor,\n",
    "    label: tf.Tensor,\n",
    "    gaussian_p: float = 0.1,\n",
    "    solarize_p: float = 0.0,\n",
    "    crop_size: int = 224,\n",
    ") -> Tuple[tf.Tensor]:\n",
    "    \"\"\"Randomly Resize and Augment Crops\"\"\"\n",
    "    # scale the pixel values\n",
    "    image, label = scale_image(image, label)\n",
    "    # image resizing\n",
    "    image_shape = 260\n",
    "    image = tf.image.resize(image, (image_shape, image_shape))\n",
    "    # get the crop from the image\n",
    "    crop = tf.image.random_crop(image, (crop_size, crop_size, 3))\n",
    "    crop_resize = tf.image.resize(crop, (crop_size, crop_size))\n",
    "    # color distortions\n",
    "    distored_image, label = custom_augment(crop_resize, label, gaussian_p)\n",
    "    return distored_image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-TyhdhWEfuoI"
   },
   "source": [
    "# 💿 The Dataset\n",
    "\n",
    "---\n",
    "\n",
    "For the purposes of this example, we use the TF Flowers dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jbKVJJKvj21x",
    "outputId": "495ace2d-a7dc-4ced-efa5-9212fe32a261"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset 218.21 MiB (download: 218.21 MiB, generated: 221.83 MiB, total: 440.05 MiB) to ~/tensorflow_datasets/tf_flowers/3.0.1...\n",
      "Dataset tf_flowers downloaded and prepared to ~/tensorflow_datasets/tf_flowers/3.0.1. Subsequent calls will reuse this data.\n"
     ]
    }
   ],
   "source": [
    "tfds.disable_progress_bar()\n",
    "\n",
    "# Gather Flowers dataset\n",
    "train_ds, validation_ds = tfds.load(\n",
    "    \"tf_flowers\", split=[\"train[:85%]\", \"train[85%:]\"], as_supervised=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bzaJxSEof6_M"
   },
   "source": [
    "## 🖖 Data Augmentation Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ctHaiHHEPuVB",
    "outputId": "ed8bec9c-beff-4a9a-db91-d9eca9c826b3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
     ]
    }
   ],
   "source": [
    "# We create a Tuple because we have two loaders corresponding to each view\n",
    "trainloaders = tuple()\n",
    "\n",
    "for i in range(NUM_VIEWS):\n",
    "    trainloader = train_ds.shuffle(1024).map(\n",
    "        lambda x, y: random_resize_crop(x, y, GAUSSIAN_P[i], SOLARIZE_P[i]),\n",
    "        num_parallel_calls=AUTOTUNE,\n",
    "    )\n",
    "    trainloader = trainloader.with_options(options)\n",
    "    trainloaders += (trainloader,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-8zt-j3Zf-la"
   },
   "source": [
    "## ⚙️ Dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zJp3ViRVQCwS"
   },
   "outputs": [],
   "source": [
    "# zip both the dataloaders together\n",
    "trainloader = tf.data.Dataset.zip(trainloaders)\n",
    "\n",
    "# final trainloader to be used for training\n",
    "trainloader = (\n",
    "    trainloader.batch(BATCH_SIZE)\n",
    "    .map(shuffle_zipped_output, num_parallel_calls=AUTOTUNE)\n",
    "    .prefetch(AUTOTUNE)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VQTV9RotiRCW"
   },
   "source": [
    "# ✍️ Model Architecture & Training\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3IsyuaYYiTka"
   },
   "source": [
    "## Building the network\n",
    "![](https://github.com/facebookresearch/vicreg/blob/main/.github/vicreg_archi_full.jpg?raw=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e3yk3MH6ifAc"
   },
   "outputs": [],
   "source": [
    "def get_resnet_backbone() -> tf.keras.Model:\n",
    "    \"\"\"Get a ResNet50 backbone to be used as a encoder\"\"\"\n",
    "    input_layer = tf.keras.layers.Input((None, None, 3))\n",
    "\n",
    "    base_model = tf.keras.applications.ResNet50(\n",
    "        include_top=False, weights=None, input_shape=(None, None, 3)\n",
    "    )\n",
    "    base_model.trainable = True\n",
    "    representations = base_model(input_layer, training=True)\n",
    "\n",
    "    output_layer = tf.keras.layers.GlobalAveragePooling2D()(representations)\n",
    "    backbone = tf.keras.models.Model(\n",
    "        inputs=input_layer, outputs=output_layer, name=\"encoder\"\n",
    "    )\n",
    "\n",
    "    return backbone\n",
    "\n",
    "\n",
    "def get_expander(num_units: int = 8192) -> tf.keras.Model:\n",
    "    \"\"\"Create a MLP to be used as a expander\"\"\"\n",
    "    input_layer = tf.keras.layers.Input((2048,))\n",
    "\n",
    "    projection_1 = tf.keras.layers.Dense(num_units)(input_layer)\n",
    "    projection_1 = tf.keras.layers.BatchNormalization()(projection_1)\n",
    "    projection_1 = tf.keras.layers.Activation(\"relu\")(projection_1)\n",
    "\n",
    "    projection_2 = tf.keras.layers.Dense(num_units)(projection_1)\n",
    "    projection_2 = tf.keras.layers.BatchNormalization()(projection_2)\n",
    "    projection_2 = tf.keras.layers.Activation(\"relu\")(projection_2)\n",
    "\n",
    "    output_layer = tf.keras.layers.Dense(num_units)(projection_2)\n",
    "\n",
    "    expander = tf.keras.models.Model(\n",
    "        inputs=input_layer, outputs=output_layer, name=\"expander\"\n",
    "    )\n",
    "\n",
    "    return expander"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oGoDjK8ooDHC"
   },
   "source": [
    "## Train Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YlkLFeC1oEF9"
   },
   "outputs": [],
   "source": [
    "def train_step(\n",
    "    input_views: Tuple,\n",
    "    encoder: tf.keras.Model,\n",
    "    expander: tf.keras.Model,\n",
    "    optimizer: Any,\n",
    "    invar_coeff: float,\n",
    "    var_coeff: float,\n",
    "    cov_coeff: float,\n",
    ") -> tf.Tensor:\n",
    "    \"\"\"Training Step\"\"\"\n",
    "    x, x_prime = input_views[0][0], input_views[1][0]\n",
    "    inputs = [x, x_prime]\n",
    "    batch_size = inputs[0][0].shape[0]\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Get Representations (through encoder)\n",
    "        y = encoder(x)\n",
    "        y_prime = encoder(x_prime)\n",
    "\n",
    "        # Get Embeddings (through expander)\n",
    "        z = expander(y)\n",
    "        z_prime = expander(y_prime)\n",
    "\n",
    "        # Calculate the Representation (Invariance) Loss\n",
    "        invar_loss = tf.keras.metrics.mean_squared_error(z, z_prime)\n",
    "\n",
    "        # Calculate var. and std. dev. of embeddings\n",
    "        z = z - tf.reduce_mean(z, axis=0)\n",
    "        z_prime = z_prime - tf.reduce_mean(z_prime, axis=0)\n",
    "        std_z = tf.sqrt(tf.math.reduce_variance(z, axis=0) + 0.0001)\n",
    "        std_z_prime = tf.sqrt(tf.math.reduce_variance(z_prime, axis=0) + 0.0001)\n",
    "\n",
    "        # Calculate the Variance Loss (Hinge Function)\n",
    "        var_loss = (\n",
    "            tf.reduce_mean(tf.nn.relu(1 - std_z)) / 2\n",
    "            + tf.reduce_mean(tf.nn.relu(1 - std_z_prime)) / 2\n",
    "        )\n",
    "\n",
    "        # Get Covariance Matrix\n",
    "        cov_z = (z.T @ z) / (batch_size - 1)\n",
    "        cov_z_prime = (z_prime.T @ z_prime) / (batch_size - 1)\n",
    "\n",
    "        # Calculate the Covariance Loss\n",
    "        cov_loss_z = tf.divide(tf.reduce_sum(tf.pow(off_diagonal(cov_z), 2)), 8192)\n",
    "        cov_loss_z_prime = tf.divide(\n",
    "            tf.reduce_sum(tf.pow(off_diagonal(cov_z_prime), 2)), 8192\n",
    "        )\n",
    "        cov_loss = cov_loss_z + cov_loss_z_prime\n",
    "\n",
    "        # Weighted Avg. of Invariance, Variance and Covariance Loss\n",
    "        loss = invar_coeff * invar_loss + var_coeff * var_loss + cov_coeff * cov_loss\n",
    "\n",
    "    variables = encoder.trainable_variables + expander.trainable_variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SOut66shzOPh"
   },
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NqZI8DyZzRI0"
   },
   "outputs": [],
   "source": [
    "def train_vicreg(\n",
    "    encoder: tf.keras.Model,\n",
    "    expander: tf.keras.Model,\n",
    "    dataloader: Any,\n",
    "    optimizer: Any,\n",
    "    invar_coeff: float,\n",
    "    var_coeff: float,\n",
    "    cov_coeff: float,\n",
    "    epochs: int,\n",
    ") -> List:\n",
    "    \"\"\"\n",
    "    Train the VICReg Model\n",
    "    \"\"\"\n",
    "\n",
    "    step_wise_loss = []\n",
    "    epoch_wise_loss = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for i, inputs in track(\n",
    "            enumerate(dataloader), total=len(dataloader), desc=f\"Epoch{epoch} |\"\n",
    "        ):\n",
    "            loss = train_step(\n",
    "                inputs, encoder, expander, optimizer, invar_coeff, var_coeff, cov_coeff\n",
    "            )\n",
    "            step_wise_loss.append(loss)\n",
    "            print(f\"step wise loss: {np.mean(loss)}\")\n",
    "        epoch_wise_loss.append(np.mean(step_wise_loss))\n",
    "\n",
    "        print(\"epoch: {} loss: {:.3f}\".format(epoch + 1, np.mean(step_wise_loss)))\n",
    "\n",
    "    return epoch_wise_loss, [encoder, expander]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5jX8kkEP59YB"
   },
   "source": [
    "## 🏃 Train !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 133,
     "referenced_widgets": [
      "fbf34c0def884eaf8be09360dc7a4f97",
      "f37e7322fd1f4dce8e4c67322fa1b8e8"
     ]
    },
    "id": "E5VIi6z3z3nU",
    "outputId": "cce1098b-c66b-42c4-bbde-6d7e2bce51db"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbf34c0def884eaf8be09360dc7a4f97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">step wise loss: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">23.193599700927734</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "step wise loss: \u001b[1;36m23.193599700927734\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">step wise loss: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">22.92543601989746</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "step wise loss: \u001b[1;36m22.92543601989746\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">step wise loss: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">22.668489456176758</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "step wise loss: \u001b[1;36m22.668489456176758\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">step wise loss: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">21.454715728759766</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "step wise loss: \u001b[1;36m21.454715728759766\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">step wise loss: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">23.38309669494629</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "step wise loss: \u001b[1;36m23.38309669494629\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">step wise loss: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">22.234115600585938</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "step wise loss: \u001b[1;36m22.234115600585938\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "encoder = get_resnet_backbone()\n",
    "expander = get_expander()\n",
    "\n",
    "# The training protocol for VICReg follows those of BYOL and Barlow Twins,\n",
    "# i.e. the use of LARS which is adaptive algorithm meant for large batch training\n",
    "lr_decayed_fn = tf.keras.optimizers.schedules.CosineDecay(\n",
    "    initial_learning_rate=BASE_LR, decay_steps=DECAY_STEPS\n",
    ")\n",
    "opt = tfm.optimization.lars_optimizer.LARS(\n",
    "    learning_rate=lr_decayed_fn, weight_decay_rate=WEIGHT_DECAY\n",
    ")\n",
    "\n",
    "epoch_wise_loss, models = train_vicreg(\n",
    "    encoder,\n",
    "    expander,\n",
    "    trainloader,\n",
    "    opt,\n",
    "    invar_coeff=INVAR_COEFF,\n",
    "    var_coeff=VAR_COEFF,\n",
    "    cov_coeff=COV_COEFF,\n",
    "    epochs=NUM_EPOCHS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tw6G77aRIez8"
   },
   "outputs": [],
   "source": [
    "encoder, expander = models\n",
    "encoder.save_weights(\"encoder_weights.h5\")\n",
    "expander.save_weights(\"expander_weights.h5\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.8 (main, Oct 13 2022, 09:48:40) [Clang 14.0.0 (clang-1400.0.29.102)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "dfc3b40ded02e54c9e1bc9b7a779d8bbf79110e7e257fe95026c8389da331d0b"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "f37e7322fd1f4dce8e4c67322fa1b8e8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fbf34c0def884eaf8be09360dc7a4f97": {
     "model_module": "@jupyter-widgets/output",
     "model_module_version": "1.0.0",
     "model_name": "OutputModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/output",
      "_model_module_version": "1.0.0",
      "_model_name": "OutputModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/output",
      "_view_module_version": "1.0.0",
      "_view_name": "OutputView",
      "layout": "IPY_MODEL_f37e7322fd1f4dce8e4c67322fa1b8e8",
      "msg_id": "",
      "outputs": [
       {
        "data": {
         "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Working... <span style=\"color: #f92672; text-decoration-color: #f92672\">━</span><span style=\"color: #3a3a3a; text-decoration-color: #3a3a3a\">╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> <span style=\"color: #800080; text-decoration-color: #800080\">  3%</span> <span style=\"color: #008080; text-decoration-color: #008080\">-:--:--</span>\n</pre>\n",
         "text/plain": "Working... \u001b[38;2;249;38;114m━\u001b[0m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m  3%\u001b[0m \u001b[36m-:--:--\u001b[0m\n"
        },
        "metadata": {},
        "output_type": "display_data"
       }
      ]
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
